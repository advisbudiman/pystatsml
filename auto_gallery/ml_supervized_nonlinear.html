
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Non-linear models &#8212; Statistics and Machine Learning in Python 0.5 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-rendered-html.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.5 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Resampling methods" href="ml_resampling.html" />
    <link rel="prev" title="Linear models for classification problems" href="../machine_learning/linear_classification.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-auto-gallery-ml-supervized-nonlinear-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="non-linear-models">
<span id="sphx-glr-auto-gallery-ml-supervized-nonlinear-py"></span><h1>Non-linear models<a class="headerlink" href="#non-linear-models" title="Permalink to this headline">¶</a></h1>
<p>Here we focuse on non-linear models for classification. Nevertheless, each
classification model has its regression counterpart.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># get_ipython().run_line_magic(&#39;matplotlib&#39;, &#39;inline&#39;)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<section id="support-vector-machines-svm">
<h2>Support Vector Machines (SVM)<a class="headerlink" href="#support-vector-machines-svm" title="Permalink to this headline">¶</a></h2>
<p>SVM are based kernel methods require only a user-specified kernel function
<span class="math notranslate nohighlight">\(K(x_i, x_j)\)</span>, i.e., a <strong>similarity function</strong> over pairs of data
points <span class="math notranslate nohighlight">\((x_i, x_j)\)</span> into kernel (dual) space on which learning
algorithms operate linearly, i.e. every operation on points is a linear
combination of <span class="math notranslate nohighlight">\(K(x_i, x_j)\)</span>.
Outline of the SVM algorithm:</p>
<ol class="arabic">
<li><p>Map points  <span class="math notranslate nohighlight">\(x\)</span> into kernel space using a kernel function:
<span class="math notranslate nohighlight">\(x \rightarrow K(x, .)\)</span>.</p></li>
<li><p>Learning algorithms operates linearly by dot product into high-kernel
space <span class="math notranslate nohighlight">\(K(., x_i) \cdot K(., x_j)\)</span>.</p>
<blockquote>
<div><ul class="simple">
<li><p>Using the kernel trick (Mercer’s Theorem) replaces dot product in high
dimensional space by a simpler operation such that
<span class="math notranslate nohighlight">\(K(., x_i) \cdot K(., x_j) = K(x_i, x_j)\)</span>.
Thus we only need to compute a similarity measure  for each pairs of
point and store in a <span class="math notranslate nohighlight">\(N \times N\)</span> Gram matrix.</p></li>
<li><p>Finally, The learning process consist of estimating the $alpha_i$ of
the decision function that maximises the hinge loss (of <span class="math notranslate nohighlight">\(f(x)\)</span>)
plus some penalty when applied on all training points.</p></li>
</ul>
</div></blockquote>
</li>
</ol>
<div class="math notranslate nohighlight">
\[f(x) = \text{sign} \left(\sum_i^N \alpha_i~y_i~K(x_i, x)\right).\]</div>
<ol class="arabic simple" start="3">
<li><p>Predict a new point $x$ using the decision function.</p></li>
</ol>
<figure class="align-default">
<img alt="Support Vector Machines." src="../_images/svm_rbf_kernel_mapping_and_decision_function.png" />
</figure>
<p>Gaussian kernel (RBF, Radial Basis Function):</p>
<p>One of the most commonly used kernel is the Radial Basis Function (RBF) Kernel.
For a pair of points <span class="math notranslate nohighlight">\(x_i, x_j\)</span> the RBF kernel is defined as:</p>
<p>Where <span class="math notranslate nohighlight">\(\sigma\)</span> (or <span class="math notranslate nohighlight">\(\gamma\)</span>)  defines the kernel width parameter.
Basically, we consider a Gaussian function centered on each training sample
<span class="math notranslate nohighlight">\(x_i\)</span>.  it has a ready interpretation as a similarity measure as it
decreases with squared Euclidean distance between the two feature vectors.</p>
<p>Non linear SVM also exists for regression problems.</p>
<p>dataset</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>Preprocessing: unequal variance of input features, requires scaling for svm.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">displot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;kde&quot;</span><span class="p">,</span> <span class="n">bw_adjust</span><span class="o">=</span><span class="mf">.2</span><span class="p">,</span> <span class="n">cut</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabels</span><span class="p">(</span><span class="s2">&quot;Std-dev&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<img alt="ml supervized nonlinear" class="sphx-glr-single-img" src="../_images/sphx_glr_ml_supervized_nonlinear_001.png" />
<p>Fit-predict
Probalility is a logistic of the decision_function</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_score</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y_score</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_prob</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_labels</span><span class="p">(</span><span class="s2">&quot;decision function&quot;</span><span class="p">,</span> <span class="s2">&quot;Probability&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<img alt="ml supervized nonlinear" class="sphx-glr-single-img" src="../_images/sphx_glr_ml_supervized_nonlinear_002.png" />
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> (AUC with proba: </span><span class="si">%.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_score</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>

<span class="c1"># Usefull internals: indices of support vectors within original X</span>
<span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">svm</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">svm</span><span class="o">.</span><span class="n">support_vectors_</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.97, AUC: 0.99 (AUC with proba: 0.99)

True
</pre></div>
</div>
</section>
<section id="random-forest">
<h2>Random forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h2>
<section id="decision-tree">
<h3>Decision tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h3>
<p>A tree can be “learned” by splitting the training dataset into subsets based on an features value test.
Each internal node represents a “test” on an feature resulting on the split of the current sample. At each step the algorithm selects the feature and a cutoff value that maximises a given metric. Different metrics exist for regression tree (target is continuous) or classification tree (the target is qualitative).
This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This general principle is implemented by many recursive partitioning tree algorithms.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/classification_tree.png"><img alt="Classification tree." src="../_images/classification_tree.png" style="width: 400px;" /></a>
</figure>
<p>Decision trees are simple to understand and interpret however they tend to overfit the data. However decision trees tend to overfit the training set.  Leo Breiman propose random forest to deal with this issue.</p>
<p>A single decision tree is usually overfits the data it is learning from because it learn from only one pathway of decisions. Predictions from a single decision tree usually don’t make accurate predictions on new data.</p>
</section>
<section id="forest">
<h3>Forest<a class="headerlink" href="#forest" title="Permalink to this headline">¶</a></h3>
<p>A random forest is a meta estimator that fits a number of <strong>decision tree learners</strong> on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.
Random forest models reduce the risk of overfitting by introducing randomness by:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/random_forest.png"><img alt="Random forest." src="../_images/random_forest.png" style="width: 300px;" /></a>
</figure>
<ul class="simple">
<li><p>building multiple trees (n_estimators)</p></li>
<li><p>drawing observations with replacement (i.e., a bootstrapped sample)</p></li>
<li><p>splitting nodes on the best split among a random subset of the features selected at every node</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.93, AUC: 0.98
</pre></div>
</div>
<p>Extra Trees (Low Variance)</p>
<p>Extra Trees is like Random Forest, in that it builds multiple trees and splits nodes using random subsets of features, but with two key differences: it does not bootstrap observations (meaning it samples without replacement), and nodes are split on random splits, not best splits. So, in summary, ExtraTrees:
builds multiple trees with bootstrap = False by default, which means it samples without replacement
nodes are split based on random splits among a random subset of the features selected at every node
In Extra Trees, randomness doesn’t come from bootstrapping of data, but rather comes from the random splits of all observations.
ExtraTrees is named for (Extremely Randomized Trees).</p>
</section>
</section>
<section id="gradient-boosting">
<h2>Gradient boosting<a class="headerlink" href="#gradient-boosting" title="Permalink to this headline">¶</a></h2>
<p>Gradient boosting is a meta estimator that fits a sequence of <strong>weak learners</strong>.
Each learner aims to reduce the residuals (errors) produced by the previous learner.
The two main hyper-parameters are:</p>
<ul class="simple">
<li><p>The <strong>learning rate</strong> (<em>lr</em>) controls over-fitting:
decreasing the <em>lr</em> limits the capacity of a learner to overfit the residuals, ie,
it slows down the learning speed and thus increases the <strong>regularisation</strong>.</p></li>
<li><p>The <strong>sub-sampling fraction</strong> controls the fraction of samples to be used for
fitting the learners. Values smaller than 1 leads to <strong>Stochastic Gradient Boosting</strong>.
It thus controls for over-fitting reducing variance and incresing bias.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/gradient_boosting.png"><img alt="Gradient boosting." src="../_images/gradient_boosting.png" style="width: 500px;" /></a>
</figure>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.94, AUC: 0.98
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.769 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-gallery-ml-supervized-nonlinear-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b52f01a0ac65cf2d2acfca70f23f792c/ml_supervized_nonlinear.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">ml_supervized_nonlinear.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/db5c6c99831700fa7c3d0b3fba90a7b4/ml_supervized_nonlinear.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">ml_supervized_nonlinear.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Statistics and Machine Learning in Python</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/python_ecosystem.html">Python ecosystem for data-science</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html#data-analysis-methodology">Data analysis methodology</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html">Import libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#basic-operations">Basic operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#execution-control-statements">Execution control statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#list-comprehensions-iterators-etc">List comprehensions, iterators, etc.</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#functions">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#regular-expression">Regular expression</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#system-programming">System programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#scripts-and-argument-parsing">Scripts and argument parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#networking">Networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#modules-and-packages">Modules and packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#object-oriented-programming-oop">Object Oriented Programming (OOP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#style-guide-for-python-programming">Style guide for Python programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#documenting">Documenting</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang.html#exercises">Exercises</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="scipy_numpy.html">Numpy: arrays and matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="scipy_pandas.html">Pandas: data manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scientific_python/scipy_matplotlib.html">Data visualization: matplotlib &amp; seaborn</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_univ.html">Univariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="stat_univ_lab_brain-volume.html">Lab: Brain volumes study</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_multiv.html">Multivariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/time_series.html">Time series in python</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../machine_learning/decomposition.html">Linear dimension reduction and feature extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine_learning/manifold.html">Manifold learning: non-linear dimension reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine_learning/clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine_learning/linear_regression.html">Linear models for regression problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine_learning/linear_classification.html">Linear models for classification problems</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Non-linear models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#support-vector-machines-svm">Support Vector Machines (SVM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#random-forest">Random forest</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradient-boosting">Gradient boosting</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ml_resampling.html">Resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine_learning/ensemble_learning.html">Ensemble learning: bagging, boosting and stacking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/optim_gradient_descent.html">Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="ml_lab_face_recognition.html">Lab: Faces recognition using various learning models</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_backprop_numpy-pytorch-sklearn.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_mlp_mnist_pytorch.html">Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_cnn_cifar10_pytorch.html">Convolutional neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_transfer-learning_cifar10-ants-bees_pytorch.html">Transfer Learning Tutorial</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../machine_learning/linear_classification.html" title="previous chapter">Linear models for classification problems</a></li>
      <li>Next: <a href="ml_resampling.html" title="next chapter">Resampling methods</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/auto_gallery/ml_supervized_nonlinear.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>