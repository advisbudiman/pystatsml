{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Non-linear models\n\nHere we focuse on non-linear models for classification. Nevertheless, each\nclassification model has its regression counterpart.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# get_ipython().run_line_magic('matplotlib', 'inline')\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nnp.set_printoptions(precision=2)\npd.set_option('precision', 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support Vector Machines (SVM)\n\nSVM are based kernel methods require only a user-specified kernel function\n$K(x_i, x_j)$, i.e., a **similarity function** over pairs of data\npoints $(x_i, x_j)$ into kernel (dual) space on which learning\nalgorithms operate linearly, i.e. every operation on points is a linear\ncombination of $K(x_i, x_j)$.\nOutline of the SVM algorithm:\n\n1. Map points  $x$ into kernel space using a kernel function:\n   $x \\rightarrow K(x, .)$.\n2. Learning algorithms operates linearly by dot product into high-kernel\n   space $K(., x_i) \\cdot K(., x_j)$.\n    - Using the kernel trick (Mercer\u2019s Theorem) replaces dot product in high\n      dimensional space by a simpler operation such that\n      $K(., x_i) \\cdot K(., x_j) = K(x_i, x_j)$.\n      Thus we only need to compute a similarity measure  for each pairs of\n      point and store in a $N \\times N$ Gram matrix.\n    - Finally, The learning process consist of estimating the $\\alpha_i$ of\n      the decision function that maximises the hinge loss (of $f(x)$)\n      plus some penalty when applied on all training points.\n\n\\begin{align}f(x) = \\text{sign} \\left(\\sum_i^N \\alpha_i~y_i~K(x_i, x)\\right).\\end{align}\n\n3. Predict a new point $x$ using the decision function.\n\n.. figure:: ../images/svm_rbf_kernel_mapping_and_decision_function.png\n   :alt: Support Vector Machines.\n\nGaussian kernel (RBF, Radial Basis Function):\n\nOne of the most commonly used kernel is the Radial Basis Function (RBF) Kernel.\nFor a pair of points $x_i, x_j$ the RBF kernel is defined as:\n\n.. raw:: latex\n\n   \\begin{align}\n      K(x_i, x_j) &= \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\\\\\n      &= \\exp\\left(-\\gamma~\\|x_i - x_j\\|^2\\right)\n   \\end{align}\n\nWhere $\\sigma$ (or $\\gamma$)  defines the kernel width parameter.\nBasically, we consider a Gaussian function centered on each training sample\n$x_i$.  it has a ready interpretation as a similarity measure as it\ndecreases with squared Euclidean distance between the two feature vectors.\n\nNon linear SVM also exists for regression problems.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.5, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing: unequal variance of input features, requires scaling for svm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = sns.displot(x=X_train.std(axis=0), kind=\"kde\", bw_adjust=.2, cut=0,\n                 fill=True, height=3, aspect=1.5,)\n_ = ax.set_xlabels(\"Std-dev\").tight_layout()\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit-predict\nProbalility is a logistic of the decision_function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel='rbf', probability=True).fit(X_train, y_train)\ny_pred = svm.predict(X_test)\ny_score = svm.decision_function(X_test)\ny_prob = svm.predict_proba(X_test)[:, 1]\n\nax = sns.relplot(x=y_score, y=y_prob, hue=y_pred, height=2, aspect=1.5)\n_ = ax.set_axis_labels(\"decision function\", \"Probability\").tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"bAcc: %.2f, AUC: %.2f (AUC with proba: %.2f)\" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_score),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))\n\n# Usefull internals: indices of support vectors within original X\nnp.all(X_train[svm.support_, :] == svm.support_vectors_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random forest\n\n### Decision tree\n\nA tree can be \"learned\" by splitting the training dataset into subsets based on an features value test.\nEach internal node represents a \"test\" on an feature resulting on the split of the current sample. At each step the algorithm selects the feature and a cutoff value that maximises a given metric. Different metrics exist for regression tree (target is continuous) or classification tree (the target is qualitative).\nThis process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This general principle is implemented by many recursive partitioning tree algorithms.\n\n.. figure:: ../images/classification_tree.png\n   :width: 400\n   :alt: Classification tree.\n\nDecision trees are simple to understand and interpret however they tend to overfit the data. However decision trees tend to overfit the training set.  Leo Breiman propose random forest to deal with this issue.\n\nA single decision tree is usually overfits the data it is learning from because it learn from only one pathway of decisions. Predictions from a single decision tree usually don\u2019t make accurate predictions on new data.\n\n### Forest\n\nA random forest is a meta estimator that fits a number of **decision tree learners** on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\nRandom forest models reduce the risk of overfitting by introducing randomness by:\n\n.. figure:: ../images/random_forest.png\n   :width: 300\n   :alt: Random forest.\n\n- building multiple trees (n_estimators)\n- drawing observations with replacement (i.e., a bootstrapped sample)\n- splitting nodes on the best split among a random subset of the features selected at every node\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators = 100)\nforest.fit(X_train, y_train)\n\ny_pred = forest.predict(X_test)\ny_prob = forest.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"bAcc: %.2f, AUC: %.2f \" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extra Trees (Low Variance)\n\nExtra Trees is like Random Forest, in that it builds multiple trees and splits nodes using random subsets of features, but with two key differences: it does not bootstrap observations (meaning it samples without replacement), and nodes are split on random splits, not best splits. So, in summary, ExtraTrees:\nbuilds multiple trees with bootstrap = False by default, which means it samples without replacement\nnodes are split based on random splits among a random subset of the features selected at every node\nIn Extra Trees, randomness doesn\u2019t come from bootstrapping of data, but rather comes from the random splits of all observations.\nExtraTrees is named for (Extremely Randomized Trees).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient boosting\n\nGradient boosting is a meta estimator that fits a sequence of **weak learners**.\nEach learner aims to reduce the residuals (errors) produced by the previous learner.\nThe two main hyper-parameters are:\n\n- The **learning rate** (*lr*) controls over-fitting:\n  decreasing the *lr* limits the capacity of a learner to overfit the residuals, ie,\n  it slows down the learning speed and thus increases the **regularisation**. \n\n- The **sub-sampling fraction** controls the fraction of samples to be used for\n  fitting the learners. Values smaller than 1 leads to **Stochastic Gradient Boosting**.\n  It thus controls for over-fitting reducing variance and incresing bias.\n\n.. figure:: ../images/gradient_boosting.png\n   :width: 500\n   :alt: Gradient boosting.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                subsample=0.5, random_state=0)\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\ny_prob = gb.predict_proba(X_test)[:, 1]\n\nprint(\"bAcc: %.2f, AUC: %.2f \" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}