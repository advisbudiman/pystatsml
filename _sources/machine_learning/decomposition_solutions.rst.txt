Dimension reduction and feature extraction
==========================================

Principal Component Analysis
----------------------------

Implement PCA
~~~~~~~~~~~~~

-  Write a class ``BasicPCA`` with two methods ``fit(X)`` that estimates
   the data mean and principal components directions. ``transform(X)``
   that project a new the data into the principal components.

-  Check that your ``BasicPCA`` performed similarly to the one from
   sklearn: ``from sklearn.decomposition import PCA``

.. code:: ipython3

    import numpy as np
    import scipy
    import matplotlib.pyplot as plt
    import seaborn as sns
    %matplotlib inline
    #%matplotlib qt
    
    np.random.seed(42)
    
    
    import numpy as np
    from sklearn.decomposition import PCA
    
    
    class BasicPCA():
        def fit(self, X):
            # U : Unitary matrix having left singular vectors as columns.
            #     Of shape (n_samples,n_samples) or (n_samples,n_comps), depending on
            #     full_matrices.
            #
            # s : The singular values, sorted in non-increasing order. Of shape (n_comps,), 
            #     with n_comps = min(n_samples, n_features).
            #
            # Vh: Unitary matrix having right singular vectors as rows. 
            #     Of shape (n_features, n_features) or (n_comps, n_features) depending on full_matrices.
            self.mean = X.mean(axis=0)
            Xc = X - self.mean  # Centering is required
            U, s, V = scipy.linalg.svd(Xc, full_matrices=False)
            self.explained_variance_ = (s ** 2) / X.shape[0]
            self.explained_variance_ratio_ = (self.explained_variance_ /
                                     self.explained_variance_.sum())
            self.princ_comp_dir = V
    
        def transform(self, X):
            Xc = X - self.mean
            return(np.dot(Xc, self.princ_comp_dir.T))
    
    # test
    np.random.seed(42)
     
    # dataset
    n_samples = 100
    experience = np.random.normal(size=n_samples)
    salary = 1500 + experience + np.random.normal(size=n_samples, scale=.5)
    X = np.column_stack([experience, salary])
    
    X = np.column_stack([experience, salary])
    pca = PCA(n_components=2)
    pca.fit(X)
    
    basic_pca = BasicPCA()
    basic_pca.fit(X)
    
    print(pca.explained_variance_ratio_)
    assert np.all(basic_pca.transform(X) == pca.transform(X))



.. parsed-literal::

    [0.93646607 0.06353393]


Apply PCA on iris dataset
~~~~~~~~~~~~~~~~~~~~~~~~~

Apply your sklearn PCA on ``iris`` dataset available at:
‘https://github.com/duchesnay/pystatsml/raw/master/datasets/iris.csv’.

.. code:: ipython3

    import matplotlib.pyplot as plt
    
    from sklearn.decomposition import PCA
    # https://tgmstat.wordpress.com/2013/11/28/computing-and-visualizing-pca-in-r/
    
    import numpy as np
    import pandas as pd
    
    try:
        salary = pd.read_csv('datasets/iris.csv')
    except:
        url = 'https://github.com/duchesnay/pystatsml/raw/master/datasets/iris.csv'
        df = pd.read_csv(url)
    
    print(df.head())


.. parsed-literal::

       sepal_length  sepal_width  petal_length  petal_width species
    0           5.1          3.5           1.4          0.2  setosa
    1           4.9          3.0           1.4          0.2  setosa
    2           4.7          3.2           1.3          0.2  setosa
    3           4.6          3.1           1.5          0.2  setosa
    4           5.0          3.6           1.4          0.2  setosa


Describe the data set. Should the dataset been standardized ?

.. code:: ipython3

    print(df.describe())


.. parsed-literal::

           sepal_length  sepal_width  petal_length  petal_width
    count    150.000000   150.000000    150.000000   150.000000
    mean       5.843333     3.057333      3.758000     1.199333
    std        0.828066     0.435866      1.765298     0.762238
    min        4.300000     2.000000      1.000000     0.100000
    25%        5.100000     2.800000      1.600000     0.300000
    50%        5.800000     3.000000      4.350000     1.300000
    75%        6.400000     3.300000      5.100000     1.800000
    max        7.900000     4.400000      6.900000     2.500000


Describe the structure of correlation among variables.

.. code:: ipython3

    X = np.array(df.iloc[:, :4])
    #np.around(np.corrcoef(X.T), 3)

.. code:: ipython3

    # Center and standardize
    
    X = np.array(df.iloc[:, :4])
    X -= np.mean(X, axis=0)
    X /= np.std(X, axis=0, ddof=1)
    np.around(np.corrcoef(X.T), 3)




.. parsed-literal::

    array([[ 1.   , -0.118,  0.872,  0.818],
           [-0.118,  1.   , -0.428, -0.366],
           [ 0.872, -0.428,  1.   ,  0.963],
           [ 0.818, -0.366,  0.963,  1.   ]])



Compute a PCA with the maximum number of components.

.. code:: ipython3

    pca = PCA(n_components=X.shape[1])
    pca.fit(X)




.. parsed-literal::

    PCA(n_components=4)



Retrieve the explained variance ratio. Determine :math:`K` the number of
components.

.. code:: ipython3

    print(pca.explained_variance_ratio_)
    
    K = 2
    pca = PCA(n_components=X.shape[1])
    pca.fit(X)
    PC = pca.transform(X)
    #print(PC)


.. parsed-literal::

    [0.72962445 0.22850762 0.03668922 0.00517871]


Print the :math:`K` principal components direction and correlation of
the :math:`K` principal components with original variables. Interpret
the contribution of original variables into the PC.

.. code:: ipython3

    print(pca.components_)
    CorPC = pd.DataFrame(
        [[np.corrcoef(X[:, j], PC[:, k])[0, 1] for j in range(X.shape[1])]
            for k in range(K)],
                columns = df.columns[:4],
        index = ["PC %i"%k for k in range(K)]
    )
    
    print(CorPC)


.. parsed-literal::

    [[ 0.52106591 -0.26934744  0.5804131   0.56485654]
     [ 0.37741762  0.92329566  0.02449161  0.06694199]
     [-0.71956635  0.24438178  0.14212637  0.63427274]
     [-0.26128628  0.12350962  0.80144925 -0.52359713]]
          sepal_length  sepal_width  petal_length  petal_width
    PC 0      0.890169    -0.460143      0.991555     0.964979
    PC 1      0.360830     0.882716      0.023415     0.064000


Plot samples projected into the :math:`K` first PCs. Color samples with
their species.

.. code:: ipython3

    colors = {'setosa':'r', 'versicolor':'g', 'virginica':'blue'}
    print(df["species"].unique())
    #plt.scatter(df['experience'], df['salary'], c=df['education'].apply(lambda x: colors[x]), s=100)
    plt.scatter(PC[:, 0], PC[:, 1], c=df["species"].apply(lambda x: colors[x]))
    plt.xlabel("PC1")
    plt.ylabel("PC2")


.. parsed-literal::

    ['setosa' 'versicolor' 'virginica']




.. parsed-literal::

    Text(0, 0.5, 'PC2')




.. image:: decomposition_solutions_files/decomposition_solutions_16_2.png


Pairewise plot

.. code:: ipython3

    import seaborn as sns
    
    df["PC1"] = PC[:, 0]
    df["PC2"] = PC[:, 1]
    
    ax = sns.pairplot(df, hue="species")



.. image:: decomposition_solutions_files/decomposition_solutions_18_0.png


