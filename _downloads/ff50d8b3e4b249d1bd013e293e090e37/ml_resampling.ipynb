{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Resampling methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import datasets\nimport sklearn.linear_model as lm\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nimport sklearn.metrics as metrics\nX, y = datasets.make_regression(n_samples=100, n_features=100,\n                                n_informative=10, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train, validation and test sets\n\nMachine learning algorithms overfit taining data. Predictive performances **MUST** be evaluated on independant hold-out dataset.\n\n.. figure:: ../images/train_val_test_cv.png\n   :alt: Train, validation and test sets.\n\n1. **Training dataset**: Dataset used to fit the model\n   (set the model parameters like weights). The *training error* can be\n   easily calculated by applying the statistical learning method to the\n   observations used in its training. But because of overfitting, the\n   **training error rate can dramatically underestimate the error** that\n   would be obtained on new samples.\n2. **Validation dataset**: Dataset used to provide an unbiased evaluation\n   of a model fit on the training dataset while\n   **tuning model hyperparameters**, ie. **model selection**.\n   The validation error is the average error that results from a learning\n   method to predict the response on a new (validation) samples that is,\n   on samples that were not used in training the method.\n3. **Test dataset**: Dataset used to provide an unbiased\n   **evaluation of a final model** fitted on the training dataset.\n   It is only used once a model is completely trained (using the train and\n   validation sets).\n\nWhat is the Difference Between Test and Validation Datasets? by\n[Jason Brownlee](https://machinelearningmastery.com/difference-test-validation-datasets/)\n\nThus the original dataset is generally split in a training, validation and a\ntest data sets. Large training+validation set (80%) small test set (20%) might\nprovide a poor estimation of the predictive performances (same argument\nstands for train vs validation samples). On the contrary, large test set and\nsmall training set might produce a poorly estimated learner.\nThis is why, on situation where we cannot afford such split, cross-validation\nscheme can be use for model selection or/and for model evaluation.\n\nIf sample size is limited, train/validation/test split may not be possible.\n**Cross Validation (CV)** (see below) can be used to replace:\n\n- Outer (train/test) split of model evaluation.\n- Inner train/validation split of model selection (more frequent situation).\n- Inner and outer splits, leading to two nested CV.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split dataset in train/test sets for model evaluation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test =\\\n    train_test_split(X, y, test_size=0.25, shuffle=True, random_state=42)\n\nmod = lm.Ridge(alpha=10)\n\nmod.fit(X_train, y_train)\n\ny_pred_test = mod.predict(X_test)\nprint(\"Test R2: %.2f\" % metrics.r2_score(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/validation/test splits: model selection and model evaluation\n\nThe **Grid search procedure** (`GridSearchCV`) performs a\nmodel selection of the best **hyper-parameters** $\\alpha$ over a grid of possible values.\nTrain set is  \"splitted (inner split) into train/validation sets.\n\n**Model selection with grid search procedure:**\n\n1. Fit the learner (\\ie. estimate **parameters** $\\mathbf{\\Omega}_k$)\n   on training set: $\\mathbf{X}_{train}, \\mathbf{y}_{train} \\rightarrow f_{\\alpha_k, \\mathbf{\\Omega}_k}(.)$\n2. Evaluate the model on the validation set and keep the hyper-parameter(s) that\n   minimises the error measure $\\alpha_* = \\arg \\min L(f_{\\alpha_k, \\mathbf{\\Omega}_k}(\\mathbf{X}_{val}), \\mathbf{y}_{val})$\n3. Refit the learner on all training + validation data,\n   $\\mathbf{X}_{train \\cup val}, \\mathbf{y}_{train \\cup val}$,\n   using the best hyper parameters ($\\alpha_*$): $\\rightarrow f_{\\alpha_*, \\mathbf{\\Omega}_*}(.)$\n\n**Model evaluation:** on the test set:\n$L(f_{\\alpha_*, \\mathbf{\\Omega}_*}(\\mathbf{X}_{test}), \\mathbf{y}_{test})$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_idx, validation_idx = train_test_split(np.arange(X_train.shape[0]),\n                                             test_size=0.25, shuffle=True,\n                                             random_state=42)\n\nsplit_inner = PredefinedSplit(test_fold=validation_idx)\nprint(\"Train set size: %i\" % X_train[train_idx].shape[0])\nprint(\"Validation set size: %i\" % X_train[validation_idx].shape[0])\nprint(\"Test set size: %i\" % X_test.shape[0])\n\nlm_cv = GridSearchCV(lm.Ridge(), {'alpha': 10. ** np.arange(-3, 3)},\n                     cv=split_inner, n_jobs=5)\n\n# Fit, indluding model selection with internal Train/validation split\nlm_cv.fit(X_train, y_train)\n\n# Predict\ny_pred_test = lm_cv.predict(X_test)\nprint(\"Test R2: %.2f\" % metrics.r2_score(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Validation (CV)\n\nIf sample size is limited, train/validation/test split may not be possible.\n**Cross Validation (CV)** can be used to replace train/validation split\nand/or train+validation / test split.\n\nCross-Validation scheme randomly divides the set of observations into\n*K* groups, or **folds**, of approximately equal size.\nThe first fold is treated as a validation set, and the method\n$f()$ is fitted on the remaining union of *K - 1* folds:\n($f(\\boldsymbol{X}_{-K}, \\boldsymbol{y}_{-K})$).\nThe measure of performance (the score function $\\mathcal{S}$),\neither a error measure or an correct prediction measure is an average\nof a loss error or correct prediction measure, noted $\\mathcal{L}$,\nbetween a true target value and the predicted target value.\nThe score function is evaluated of the on the observations in the held-out\nfold. For each sample *i* we consider the model estimated\n$f(\\boldsymbol{X}_{-k(i)}, \\boldsymbol{y}_{-k(i)}$ on the data set\nwithout the group *k* that contains *i* noted *-k(i)*.\nThis procedure is repeated *K* times; each time, a different group of\nobservations is treated as a test set.\nThen we compare the predicted value\n($f_{-k(i)}(\\boldsymbol{x}_i) = \\hat{y_i})$\nwith true value $y_i$ using a Error or Loss function\n$\\mathcal{L}(y, \\hat{y})$.\n\nFor 10-fold we can either average over 10 values (Macro measure) or\nconcatenate the 10 experiments and compute the micro measures.\n\nTwo strategies [micro vs macro estimates](https://stats.stackexchange.com/questions/34611/meanscores-vs-scoreconcatenation-in-cross-validation):\n\n- **Micro measure: average(individual scores)**: compute a score\n  $\\mathcal{S}$ for each sample and average over all samples.\n  It is simillar to **average score(concatenation)**: an averaged score\n  computed over all concatenated samples.\n\n.. raw:: latex\n   \\mathcal{S}(f) = \\frac{1}{N} \\sum_i^N \\mathcal{L}\\left(y_i, f(\\boldsymbol{x}_{-k(i)}, \\boldsymbol{y}_{-k(i)}) \\right).\n\n- **Macro measure mean(CV scores)** (the most commonly used method):\n  compute a score $\\mathcal{S}$ on each each fold *k* and average\n  accross folds:\n\n.. raw:: latex\n   \\begin{align*}\n   \\mathcal{S}(f) &= \\frac{1}{K} \\sum_k^K \\mathcal{S}_k(f).\\\\\n   \\mathcal{S}(f) &= \\frac{1}{K} \\sum_k^K \\frac{1}{N_k} \\sum_{i \\in k} \\mathcal{L}\\left(y_i, f(\\boldsymbol{x}_{-k(i)}, \\boldsymbol{y}_{-k(i)}) \\right).\n   \\end{align*}\n\nThese two measures (an average of average vs. a global average) are generaly\nsimilar. They may differ slightly is folds are of different sizes.\nThis validation scheme is known as the **K-Fold CV**.\nTypical choices of *K* are 5 or 10, [Kohavi 1995].\nThe extreme case where *K = N* is known as **leave-one-out cross-validation,\nLOO-CV**.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CV for regression\n\nUsually the error function $\\mathcal{L}()$ is the r-squared score.\nHowever other function (MAE, MSE) can be used.\n\n**CV with explicit loop:**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n\nestimator = lm.Ridge(alpha=10)\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nr2_train, r2_test = list(), list()\n\nfor train, test in cv.split(X):\n    estimator.fit(X[train, :], y[train])\n    r2_train.append(metrics.r2_score(y[train], estimator.predict(X[train, :])))\n    r2_test.append(metrics.r2_score(y[test], estimator.predict(X[test, :])))\n\nprint(\"Train r2:%.2f\" % np.mean(r2_train))\nprint(\"Test  r2:%.2f\" % np.mean(r2_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-learn provides user-friendly function to perform CV:\n\n`cross_val_score()`: single metric\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(estimator=estimator, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(estimator=estimator, X=X, y=y, cv=cv)\nprint(\"Test  r2:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`cross_validate()`: multi metric, + time, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\nscores = cross_validate(estimator=mod, X=X, y=y, cv=cv,\n                        scoring=['r2', 'neg_mean_absolute_error'])\n\nprint(\"Test R2:%.2f; MAE:%.2f\" % (scores['test_r2'].mean(),\n                                  -scores['test_neg_mean_absolute_error'].mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CV for classification: stratifiy for the target label\n\nWith classification problems it is essential to sample folds where each\nset contains approximately the same percentage of samples of each target\nclass as the complete set. This is called **stratification**.\nIn this case, we will use ``StratifiedKFold`` with is a variation of\nk-fold which returns stratified folds.\nUsually the error function $L()$ are, at least, the sensitivity\nand the specificity. However other function could be used.\n\n**CV with explicit loop**:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n\nX, y = datasets.make_classification(n_samples=100, n_features=100, shuffle=True,\n                                    n_informative=10, random_state=42)\n\nmod = lm.LogisticRegression(C=1, solver='lbfgs')\n\ncv = StratifiedKFold(n_splits=5)\n\n# Lists to store scores by folds (for macro measure only)\nbacc, auc = [], []\n\nfor train, test in cv.split(X, y):\n    mod.fit(X[train, :], y[train])\n    bacc.append(metrics.roc_auc_score(y[test], mod.decision_function(X[test, :])))\n    auc.append(metrics.balanced_accuracy_score(y[test], mod.predict(X[test, :])))\n\nprint(\"Test AUC:%.2f; bACC:%.2f\" % (np.mean(bacc), np.mean(auc)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`cross_val_score()`: single metric\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(estimator=mod, X=X, y=y, cv=5)\n\nprint(\"Test  ACC:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provide your own CV and score\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def balanced_acc(estimator, X, y, **kwargs):\n    \"\"\"Balanced acuracy scorer.\"\"\"\n    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\n\nscores = cross_val_score(estimator=mod, X=X, y=y, cv=cv,\n                         scoring=balanced_acc)\nprint(\"Test  bACC:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`cross_validate()`: multi metric, + time, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\nscores = cross_validate(estimator=mod, X=X, y=y, cv=cv,\n                        scoring=['balanced_accuracy', 'roc_auc'])\n\nprint(\"Test AUC:%.2f; bACC:%.2f\" % (scores['test_roc_auc'].mean(),\n                                    scores['test_balanced_accuracy'].mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validation for model selection\n\nCombine CV and grid search:\nRe-split (inner split) train set into CV folds train/validation folds and\nbuild a `GridSearchCV` out of it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Outer split:\nX_train, X_test, y_train, y_test =\\\n    train_test_split(X, y, test_size=0.25, shuffle=True, random_state=42)\n\ncv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Cross-validation for model selection\nlm_cv = GridSearchCV(lm.LogisticRegression(), {'C': 10. ** np.arange(-3, 3)},\n                     cv=cv_inner, n_jobs=5)\n\n# Fit, indluding model selection with internal CV\nlm_cv.fit(X_train, y_train)\n\n# Predict\ny_pred_test = lm_cv.predict(X_test)\nprint(\"Test bACC: %.2f\" % metrics.balanced_accuracy_score(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validation for both model (outer) evaluation and model (inner) selection\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Cross-validation for model (inner) selection\nlm_cv = GridSearchCV(lm.Ridge(), {'alpha': 10. ** np.arange(-3, 3)},\n                     cv=cv_inner, n_jobs=5)\n\n# Cross-validation for model (outer) evaluation\nscores = cross_validate(estimator=mod, X=X, y=y, cv=cv_outer,\n                        scoring=['balanced_accuracy', 'roc_auc'])\n\nprint(\"Test AUC:%.2f; bACC:%.2f, Time: %.2fs\" % (scores['test_roc_auc'].mean(),\n                                        scores['test_balanced_accuracy'].mean(),\n                                        scores['fit_time'].sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models with built-in cross-validation\n\nLet sklearn select the best parameters over a default grid.\n\n**Classification**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"== Logistic Ridge (L2 penalty) ==\")\nmod_cv = lm.LogisticRegressionCV(class_weight='balanced', scoring='balanced_accuracy',\n                                 n_jobs=-1, cv=5)\nscores = cross_val_score(estimator=mod_cv, X=X, y=y, cv=5)\nprint(\"Test  ACC:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Regression**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, coef = datasets.make_regression(n_samples=50, n_features=100, noise=10,\n                         n_informative=2, random_state=42, coef=True)\n\nprint(\"== Ridge (L2 penalty) ==\")\nmodel = lm.RidgeCV(cv=3)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\n\nprint(\"== Lasso (L1 penalty) ==\")\nmodel = lm.LassoCV(n_jobs=-1, cv=3)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\n\nprint(\"== ElasticNet (L1 penalty) ==\")\nmodel = lm.ElasticNetCV(l1_ratio=[.1, .5, .9], n_jobs=-1, cv=3)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Permutations: sample the null distribution\n\nA permutation test is a type of non-parametric randomization test in which the null distribution of a test statistic is estimated by randomly permuting the observations.\n\nPermutation tests are highly attractive because they make no assumptions other than that the observations are independent and identically distributed under the null hypothesis.\n\n1. Compute a observed statistic $t_{obs}$ on the data.\n2. Use randomization to compute the distribution of $t$ under the null hypothesis: Perform $N$ random permutation of the data. For each sample of permuted data, $i$ the data compute the statistic $t_i$. This procedure provides the distribution of *t* under the null hypothesis $H_0$: $P(t \\vert H_0)$\n3. Compute the p-value = $P(t>t_{obs} | H_0) \\left\\vert\\{t_i > t_{obs}\\}\\right\\vert$, where $t_i's include :math:$t_{obs}`.\n\nExample Ridge regression\n\nSample the distributions of r-squared and coefficients of ridge regression under the null hypothesis. Simulated dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Regression dataset where first 2 features are predictives\nnp.random.seed(0)\nn_features = 5\nn_features_info = 2\nn_samples = 100\nX = np.random.randn(100, 5)\nbeta = np.zeros(n_features)\nbeta[:n_features_info] = 1\nXbeta = np.dot(X, beta)\neps = np.random.randn(n_samples)\ny = Xbeta + eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random permutations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fit model on all data (!! risk of overfit)\nmodel = lm.RidgeCV()\nmodel.fit(X, y)\nprint(\"Coefficients on all data:\")\nprint(model.coef_)\n\n# Random permutation loop\nnperm = 1000  # !! Should be at least 1000 (to assess a p-value at 1%)\nscores_names = [\"r2\"]\nscores_perm = np.zeros((nperm + 1, len(scores_names)))\ncoefs_perm = np.zeros((nperm + 1, X.shape[1]))\n\nscores_perm[0, :] = metrics.r2_score(y, model.predict(X))\ncoefs_perm[0, :] = model.coef_\n\norig_all = np.arange(X.shape[0])\nfor perm_i in range(1, nperm + 1):\n    model.fit(X, np.random.permutation(y))\n    y_pred = model.predict(X).ravel()\n    scores_perm[perm_i, :] = metrics.r2_score(y, y_pred)\n    coefs_perm[perm_i, :] = model.coef_\n\n# One-tailed empirical p-value\npval_pred_perm = np.sum(scores_perm >= scores_perm[0]) / scores_perm.shape[0]\npval_coef_perm = np.sum(coefs_perm >= coefs_perm[0, :], axis=0) / coefs_perm.shape[0]\n\nprint(\"R2 p-value: %.3f\" % pval_pred_perm)\nprint(\"Coeficients p-values:\", np.round(pval_coef_perm, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute p-values corrected for multiple comparisons using FWER max-T\n(Westfall and Young, 1993) procedure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pval_coef_perm_tmax = np.array([np.sum(coefs_perm.max(axis=1) >= coefs_perm[0, j])\n                                for j in range(coefs_perm.shape[1])]) / coefs_perm.shape[0]\nprint(\"P-values with FWER (Westfall and Young) correction\")\nprint(pval_coef_perm_tmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot distribution of third coefficient under null-hypothesis\nCoeffitients 0 and 1 are significantly different from 0.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def hist_pvalue(perms, ax, name):\n    \"\"\"Plot statistic distribution as histogram.\n\n    Paramters\n    ---------\n    perms: 1d array, statistics under the null hypothesis.\n           perms[0] is the true statistic .\n    \"\"\"\n    # Re-weight to obtain distribution\n    pval = np.sum(perms >= perms[0]) / perms.shape[0]\n    weights = np.ones(perms.shape[0]) / perms.shape[0]\n    ax.hist([perms[perms >= perms[0]], perms], histtype='stepfilled',\n             bins=100, label=\"p-val<%.3f\" % pval,\n             weights=[weights[perms >= perms[0]], weights])\n    ax.axvline(x=perms[0], color=\"k\", linewidth=2)#, label=\"observed statistic\")\n    ax.set_ylabel(name)\n    ax.legend()\n    return ax\n\nn_coef = coefs_perm.shape[1]\nfig, axes = plt.subplots(n_coef, 1, figsize=(12, 9))\nfor i in range(n_coef):\n    hist_pvalue( coefs_perm[:, i], axes[i], str(i))\n\n_ = axes[-1].set_xlabel(\"Coefficient distribution under null hypothesis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exercise\n\nGiven the logistic regression presented above and its validation given a 5 folds CV.\n\n1. Compute the p-value associated with the prediction accuracy measured with 5CV using a permutation test.\n\n2. Compute the p-value associated with the prediction accuracy using a parametric test.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bootstrapping\n\nBootstrapping is a statistical technique which consists in generating sample (called bootstrap samples) from an initial dataset of size *N* by randomly drawing with replacement *N* observations. It provides sub-samples with the same distribution than the original dataset. It aims to:\n\n1. Assess the variability (standard error, [confidence intervals.](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html#the-bootstrap-method-and-empirical-confidence-intervals)) of performances scores or estimated parameters (see Efron et al. 1986).\n2. Regularize model by fitting several models on bootstrap samples and averaging their predictions (see Baging and random-forest).\n\nA great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients.\n\n1. Perform $B$ sampling, with replacement, of the dataset.\n2. For each sample $i$ fit the model and compute the scores.\n3. Assess standard errors and confidence intervals of scores using the scores obtained on the $B$ resampled dataset. Or, average models predictions.\n\nReferences:\n\n[Efron B, Tibshirani R. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Stat Sci 1986;1:54\u201375](https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Bootstrap loop\nnboot = 100  # !! Should be at least 1000\nscores_names = [\"r2\"]\nscores_boot = np.zeros((nboot, len(scores_names)))\ncoefs_boot = np.zeros((nboot, X.shape[1]))\n\norig_all = np.arange(X.shape[0])\nfor boot_i in range(nboot):\n    boot_tr = np.random.choice(orig_all, size=len(orig_all), replace=True)\n    boot_te = np.setdiff1d(orig_all, boot_tr, assume_unique=False)\n    Xtr, ytr = X[boot_tr, :], y[boot_tr]\n    Xte, yte = X[boot_te, :], y[boot_te]\n    model.fit(Xtr, ytr)\n    y_pred = model.predict(Xte).ravel()\n    scores_boot[boot_i, :] = metrics.r2_score(yte, y_pred)\n    coefs_boot[boot_i, :] = model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute Mean, SE, CI\nCoeffitients 0 and 1 are significantly different from 0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores_boot = pd.DataFrame(scores_boot, columns=scores_names)\nscores_stat = scores_boot.describe(percentiles=[.975, .5, .025])\n\nprint(\"r-squared: Mean=%.2f, SE=%.2f, CI=(%.2f %.2f)\" %      tuple(scores_stat.loc[[\"mean\", \"std\", \"2.5%\", \"97.5%\"], \"r2\"]))\n\ncoefs_boot = pd.DataFrame(coefs_boot)\ncoefs_stat = coefs_boot.describe(percentiles=[.975, .5, .025])\nprint(\"Coefficients distribution\")\nprint(coefs_stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot coefficient distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(coefs_boot)\nstaked = pd.melt(df, var_name=\"Variable\", value_name=\"Coef. distribution\")\nsns.set_theme(style=\"whitegrid\")\nax = sns.violinplot(x=\"Variable\", y=\"Coef. distribution\", data=staked)\n_ = ax.axhline(0, ls='--', lw=2, color=\"black\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel computation with joblib\n\nDataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn import datasets\nimport sklearn.linear_model as lm\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import StratifiedKFold\nX, y = datasets.make_classification(n_samples=20, n_features=5, n_informative=2, random_state=42)\ncv = StratifiedKFold(n_splits=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use `cross_validate` function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\nestimator = lm.LogisticRegression(C=1, solver='lbfgs')\ncv_results = cross_validate(estimator, X, y, cv=cv, n_jobs=5)\nprint(np.mean(cv_results['test_score']), cv_results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sequential computation\n\nIf we want have full control of the operations performed within each fold (retrieve the models parameters, etc.). We would like to parallelize the folowing sequetial code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In[22]:\n\n\nestimator = lm.LogisticRegression(C=1, solver='lbfgs')\ny_test_pred_seq = np.zeros(len(y)) # Store predictions in the original order\ncoefs_seq = list()\nfor train, test in cv.split(X, y):\n    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]\n    estimator.fit(X_train, y_train)\n    y_test_pred_seq[test] = estimator.predict(X_test)\n    coefs_seq.append(estimator.coef_)\n\ntest_accs = [metrics.accuracy_score(y[test], y_test_pred_seq[test]) for train, test in cv.split(X, y)]\nprint(np.mean(test_accs), test_accs)\ncoefs_cv = np.array(coefs_seq)\nprint(coefs_cv)\n\nprint(coefs_cv.mean(axis=0))\nprint(\"Std Err of the coef\")\nprint(coefs_cv.std(axis=0) / np.sqrt(coefs_cv.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parallel computation with joblib\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\nfrom sklearn.base import is_classifier, clone\n\ndef _split_fit_predict(estimator, X, y, train, test):\n    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]\n    estimator.fit(X_train, y_train)\n    return [estimator.predict(X_test), estimator.coef_]\n\nestimator = lm.LogisticRegression(C=1, solver='lbfgs')\n\nparallel = Parallel(n_jobs=5)\ncv_ret = parallel(\n    delayed(_split_fit_predict)(\n        clone(estimator), X, y, train, test)\n    for train, test in cv.split(X, y))\n\ny_test_pred_cv, coefs_cv = zip(*cv_ret)\n\n# Retrieve predictions in the original order\ny_test_pred = np.zeros(len(y))\nfor i, (train, test) in enumerate(cv.split(X, y)):\n    y_test_pred[test] = y_test_pred_cv[i]\n\ntest_accs = [metrics.accuracy_score(y[test], y_test_pred[test]) for train, test in cv.split(X, y)]\nprint(np.mean(test_accs), test_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test same predictions and same coeficients\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert np.all(y_test_pred == y_test_pred_seq)\nassert np.allclose(np.array(coefs_cv).squeeze(), np.array(coefs_seq).squeeze())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}