
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Linear models for regression problems &#8212; Statistics and Machine Learning in Python 0.5 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-rendered-html.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.5 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear models for classification problems" href="linear_classification.html" />
    <link rel="prev" title="Clustering" href="clustering.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="linear-models-for-regression-problems">
<h1>Linear models for regression problems<a class="headerlink" href="#linear-models-for-regression-problems" title="Permalink to this headline">¶</a></h1>
<figure class="align-default" id="id1">
<img alt="Linear regression" src="../_images/linear_regression.png" />
<figcaption>
<p><span class="caption-text">Linear regression</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="ordinary-least-squares">
<h2>Ordinary least squares<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h2>
<p>Linear regression models the <strong>output</strong>, or <strong>target</strong> variable
<span class="math notranslate nohighlight">\(y \in \mathrm{R}\)</span> as a linear combination of the
<span class="math notranslate nohighlight">\(P\)</span>-dimensional input <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{P}\)</span>. Let
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be the <span class="math notranslate nohighlight">\(N \times P\)</span> matrix with each row an
input vector (with a 1 in the first position), and similarly let
<span class="math notranslate nohighlight">\(\mathbf{y}\)</span> be the <span class="math notranslate nohighlight">\(N\)</span>-dimensional vector of outputs in the
<strong>training set</strong>, the linear model will predict the <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>
given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> using the <strong>parameter vector</strong>, or <strong>weight
vector</strong> <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^P\)</span> according to</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X} \mathbf{w} + \boldsymbol{\varepsilon},\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \in \mathrm{R}^N\)</span> are the
<strong>residuals</strong>, or the errors of the prediction. The <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>
is found by minimizing an <strong>objective function</strong>, which is the <strong>loss
function</strong>, <span class="math notranslate nohighlight">\(L(\mathbf{w})\)</span>, i.e. the error measured on the data.
This error is the <strong>sum of squared errors (SSE) loss</strong>.</p>
<p>Minimizing the SSE is the Ordinary Least Square <strong>OLS</strong> regression as
objective function. which is a simple <strong>ordinary least squares (OLS)</strong>
minimization whose analytic solution is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}_{\text{OLS}} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\]</div>
<p>The gradient of the loss:</p>
<div class="math notranslate nohighlight">
\[\partial\frac{L(\mathbf{w}, \mathbf{X}, \mathbf{y})}{\partial\mathbf{w}} = 2 \sum_i \mathbf{x}_i (\mathbf{x}_i \cdot \mathbf{w} - y_i)\]</div>
</section>
<section id="linear-regression-with-scikit-learn">
<h2>Linear regression with scikit-learn<a class="headerlink" href="#linear-regression-with-scikit-learn" title="Permalink to this headline">¶</a></h2>
<p>Scikit learn offer many models for supervised learning, and they all
follow the same application programming interface (API), namely:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Estimator</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Linear regression of <code class="docutils literal notranslate"><span class="pre">Advertising.csv</span></code> dataset with TV and Radio
advertising as input features and Sales as target. The linear model that
minimizes the MSE is a plan (2 input features) defined as: Sales = 0.05
TV + .19 Radio + 3:</p>
<figure class="align-default" id="id2">
<img alt="Linear regression" src="../_images/linear_regression_plan.png" />
<figcaption>
<p><span class="caption-text">Linear regression</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="overfitting">
<h2>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">¶</a></h2>
<p>In statistics and machine learning, overfitting occurs when a
statistical model describes random errors or noise instead of the
underlying relationships. Overfitting generally occurs when a model is
<strong>excessively complex</strong>, such as having <strong>too many parameters relative
to the number of observations</strong>. A model that has been overfit will
generally have poor predictive performance, as it can exaggerate minor
fluctuations in the data.</p>
<p>A learning algorithm is trained using some set of training samples. If
the learning algorithm has the capacity to overfit the training samples
the performance on the <strong>training sample set</strong> will improve while the
performance on unseen <strong>test sample set</strong> will decline.</p>
<p>The overfitting phenomenon has three main explanations: - excessively
complex models, - multicollinearity, and - high dimensionality.</p>
<section id="model-complexity">
<h3>Model complexity<a class="headerlink" href="#model-complexity" title="Permalink to this headline">¶</a></h3>
<p>Complex learners with too many parameters relative to the number of
observations may overfit the training dataset.</p>
</section>
<section id="multicollinearity">
<h3>Multicollinearity<a class="headerlink" href="#multicollinearity" title="Permalink to this headline">¶</a></h3>
<p>Predictors are highly correlated, meaning that one can be linearly
predicted from the others. In this situation the coefficient estimates
of the multiple regression may change erratically in response to small
changes in the model or the data. Multicollinearity does not reduce the
predictive power or reliability of the model as a whole, at least not
within the sample data set; it only affects computations regarding
individual predictors. That is, a multiple regression model with
correlated predictors can indicate how well the entire bundle of
predictors predicts the outcome variable, but it may not give valid
results about any individual predictor, or about which predictors are
redundant with respect to others. In case of perfect multicollinearity
the predictor matrix is singular and therefore cannot be inverted. Under
these circumstances, for a general linear model
<span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{X} \mathbf{w} + \boldsymbol{\varepsilon}\)</span>,
the ordinary least-squares estimator,
<span class="math notranslate nohighlight">\(\mathbf{w}_{OLS} = (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}\)</span>,
does not exist.</p>
<p>An example where correlated predictor may produce an unstable model
follows: We want to predict the business potential (pb) of some
companies given their business volume (bv) and the taxes (tx) they are
paying. Here pb ~ 10% of bv. However, taxes = 20% of bv (tax and bv are
highly collinear), therefore there is an infinite number of linear
combinations of tax and bv that lead to the same prediction. Solutions
with very large coefficients will produce excessively large predictions.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>             <span class="c1"># business volume</span>
<span class="n">tax</span>  <span class="o">=</span> <span class="mf">.2</span> <span class="o">*</span> <span class="n">bv</span>                                  <span class="c1"># Tax</span>
<span class="n">bp</span> <span class="o">=</span> <span class="mf">.1</span> <span class="o">*</span> <span class="n">bv</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">.2</span><span class="p">,</span> <span class="mf">.1</span><span class="p">])</span> <span class="c1"># business potential</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">bv</span><span class="p">,</span> <span class="n">tax</span><span class="p">])</span>
<span class="n">beta_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># true solution</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">Since tax and bv are correlated, there is an infinite number of linear combinations</span>
<span class="sd">leading to the same prediction.</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="c1"># 10 times the bv then subtract it 9 times using the tax variable:</span>
<span class="n">beta_medium</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.1</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mf">.1</span> <span class="o">*</span> <span class="mi">9</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">.2</span><span class="p">)])</span>
<span class="c1"># 100 times the bv then subtract it 99 times using the tax variable:</span>
<span class="n">beta_large</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.1</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mf">.1</span> <span class="o">*</span> <span class="mi">99</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mf">.2</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L2 norm of coefficients: small:</span><span class="si">%.2f</span><span class="s2">, medium:</span><span class="si">%.2f</span><span class="s2">, large:</span><span class="si">%.2f</span><span class="s2">.&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">beta_star</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">beta_medium</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">beta_large</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;However all models provide the exact same predictions.&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_star</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_medium</span><span class="p">))</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_star</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_large</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">L2</span> <span class="n">norm</span> <span class="n">of</span> <span class="n">coefficients</span><span class="p">:</span> <span class="n">small</span><span class="p">:</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">medium</span><span class="p">:</span><span class="mf">21.25</span><span class="p">,</span> <span class="n">large</span><span class="p">:</span><span class="mf">2550.25</span><span class="o">.</span>
<span class="n">However</span> <span class="nb">all</span> <span class="n">models</span> <span class="n">provide</span> <span class="n">the</span> <span class="n">exact</span> <span class="n">same</span> <span class="n">predictions</span><span class="o">.</span>
</pre></div>
</div>
<p>Multicollinearity between the predictors: business volumes and tax
produces unstable models with arbitrary large coefficients.
<img alt="Multicollinearity between the predictors" src="../_images/ols_multicollinearity.png" /></p>
<p>Dealing with multicollinearity:</p>
<ul class="simple">
<li><p>Regularisation by e.g. <span class="math notranslate nohighlight">\(\ell_2\)</span> shrinkage: Introduce a bias in
the solution by making <span class="math notranslate nohighlight">\((X^T X)^{-1}\)</span> non-singular. See
<span class="math notranslate nohighlight">\(\ell_2\)</span> shrinkage.</p></li>
<li><p>Feature selection: select a small number of features. See: Isabelle
Guyon and André Elisseeff <em>An introduction to variable and feature
selection</em> The Journal of Machine Learning Research, 2003.</p></li>
<li><p>Feature selection: select a small number of features using
<span class="math notranslate nohighlight">\(\ell_1\)</span> shrinkage.</p></li>
<li><p>Extract few independent (uncorrelated) features using e.g. principal
components analysis (PCA), partial least squares regression (PLS-R)
or regression methods that cut the number of predictors to a smaller
set of uncorrelated components.</p></li>
</ul>
</section>
<section id="high-dimensionality">
<h3>High dimensionality<a class="headerlink" href="#high-dimensionality" title="Permalink to this headline">¶</a></h3>
<p>High dimensions means a large number of input features. Linear predictor
associate one parameter to each input feature, so a high-dimensional
situation (<span class="math notranslate nohighlight">\(P\)</span>, number of features, is large) with a relatively
small number of samples <span class="math notranslate nohighlight">\(N\)</span> (so-called large <span class="math notranslate nohighlight">\(P\)</span> small
<span class="math notranslate nohighlight">\(N\)</span> situation) generally lead to an overfit of the training data.
Thus it is generally a bad idea to add many input features into the
learner. This phenomenon is called the <strong>curse of dimensionality</strong>.</p>
<p>One of the most important criteria to use when choosing a learning
algorithm is based on the relative size of <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(N\)</span>.</p>
<ul class="simple">
<li><p>Remenber that the “covariance” matrix <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span>
used in the linear model is a <span class="math notranslate nohighlight">\(P \times P\)</span> matrix of rank
<span class="math notranslate nohighlight">\(\min(N, P)\)</span>. Thus if <span class="math notranslate nohighlight">\(P &gt; N\)</span> the equation system is
overparameterized and admit an infinity of solutions that might be
specific to the learning dataset. See also ill-conditioned or
singular matrices.</p></li>
<li><p>The sampling density of <span class="math notranslate nohighlight">\(N\)</span> samples in an <span class="math notranslate nohighlight">\(P\)</span>-dimensional
space is proportional to <span class="math notranslate nohighlight">\(N^{1/P}\)</span>. Thus a high-dimensional
space becomes very sparse, leading to poor estimations of samples
densities. To preserve a constant density, an exponential growth in
the number of observations is required. 50 points in 1D, would
require 2 500 points in 2D and 125 000 in 3D!</p></li>
<li><p>Another consequence of the sparse sampling in high dimensions is that
all sample points are close to an edge of the sample. Consider
<span class="math notranslate nohighlight">\(N\)</span> data points uniformly distributed in a
<span class="math notranslate nohighlight">\(P\)</span>-dimensional unit ball centered at the origin. Suppose we
consider a nearest-neighbor estimate at the origin. The median
distance from the origin to the closest data point is given by the
expression:
<span class="math notranslate nohighlight">\(d(P, N) = \left(1 - \frac{1}{2}^{1/N}\right)^{1/P}.\)</span></p></li>
</ul>
<p>A more complicated expression exists for the mean distance to the
closest point. For N = 500, P = 10 , <span class="math notranslate nohighlight">\(d(P, N ) \approx 0.52\)</span>, more
than halfway to the boundary. Hence most data points are closer to the
boundary of the sample space than to any other data point. The reason
that this presents a problem is that prediction is much more difficult
near the edges of the training sample. One must extrapolate from
neighboring sample points rather than interpolate between them.
<em>(Source: T Hastie, R Tibshirani, J Friedman.</em>The Elements of
Statistical Learning: Data Mining, Inference, and Prediction.* Second
Edition, 2009.)*</p>
<ul class="simple">
<li><p>Structural risk minimization provides a theoretical background of
this phenomenon. (See VC dimension.)</p></li>
<li><p>See also bias–variance trade-off.</p></li>
</ul>
</section>
</section>
<section id="regularization-using-penalization-of-coefficients">
<h2>Regularization using penalization of coefficients<a class="headerlink" href="#regularization-using-penalization-of-coefficients" title="Permalink to this headline">¶</a></h2>
<p>Regarding linear models, overfitting generally leads to excessively
complex solutions (coefficient vectors), accounting for noise or
spurious correlations within predictors. <strong>Regularization</strong> aims to
alleviate this phenomenon by constraining (biasing or reducing) the
capacity of the learning algorithm in order to promote simple solutions.
Regularization penalizes “large” solutions forcing the coefficients to
be small, i.e. to shrink them toward zeros.</p>
<p>The objective function <span class="math notranslate nohighlight">\(J(\mathbf{w})\)</span> to minimize with respect to
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is composed of a loss function <span class="math notranslate nohighlight">\(L(\mathbf{w})\)</span>
for goodness-of-fit and a penalty term <span class="math notranslate nohighlight">\(\Omega(\mathbf{w})\)</span>
(regularization to avoid overfitting). This is a trade-off where the
respective contribution of the loss and the penalty terms is controlled
by the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>Therefore the <strong>loss function</strong> <span class="math notranslate nohighlight">\(L(\mathbf{w})\)</span> is combined with a
<strong>penalty function</strong> <span class="math notranslate nohighlight">\(\Omega(\mathbf{w})\)</span> leading to the general
form:</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}) = L(\mathbf{w}) + \lambda \Omega(\mathbf{w}).\]</div>
<p>The respective contribution of the loss and the penalty is controlled by
the <strong>regularization parameter</strong> <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>For regression problems the loss is the SSE given by:</p>
<p>Popular penalties are:</p>
<ul class="simple">
<li><p>Ridge (also called <span class="math notranslate nohighlight">\(\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_2^2\)</span>. It shrinks coefficients toward 0.</p></li>
<li><p>Lasso (also called <span class="math notranslate nohighlight">\(\ell_1\)</span>) penalty: <span class="math notranslate nohighlight">\(\|\mathbf{w}\|_1\)</span>.
It performs feature selection by setting some coefficients to 0.</p></li>
<li><p>ElasticNet (also called <span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\alpha \left(\rho~\|\mathbf{w}\|_1 + (1-\rho)~\|\mathbf{w}\|_2^2 \right)\)</span>.
It performs selection of group of correlated features by setting some
coefficients to 0.</p></li>
</ul>
<p>The next figure shows the predicted performance (r-squared) on train and
test sets with an increasing number of input features. The number of
predictive features is always 10% of the total number of input features.
Therefore, the signal to noise ratio (SNR) increases by increasing the
number of input features. The performances on the training set rapidly
reach 100% (R2=1). However, the performance on the test set decreases
with the increase of the input dimensionality. The difference between
the train and test performances (blue shaded region) depicts the
overfitting phenomena. Regularisation using penalties of the coefficient
vector norm greatly limits the overfitting phenomena.</p>
<figure class="align-default" id="id3">
<img alt="Multicollinearity between the predictors" src="../_images/linear_regression_penalties.png" />
<figcaption>
<p><span class="caption-text">Multicollinearity between the predictors</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>With scikit-learn:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset with some correlation</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                      <span class="n">effective_rank</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda is alpha!</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda is alpha !</span>

<span class="n">l1l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">.9</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">coef</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">)),</span>
             <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;True&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l1l2&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>True</th>
      <td>28.49</td>
      <td>0.00e+00</td>
      <td>13.17</td>
      <td>0.00e+00</td>
      <td>48.97</td>
      <td>70.44</td>
      <td>39.70</td>
      <td>0.00e+00</td>
      <td>0.00e+00</td>
      <td>0.00e+00</td>
    </tr>
    <tr>
      <th>lr</th>
      <td>28.49</td>
      <td>3.18e-14</td>
      <td>13.17</td>
      <td>-3.05e-14</td>
      <td>48.97</td>
      <td>70.44</td>
      <td>39.70</td>
      <td>-2.48e-14</td>
      <td>4.94e-14</td>
      <td>-2.63e-14</td>
    </tr>
    <tr>
      <th>l2</th>
      <td>1.03</td>
      <td>2.11e-01</td>
      <td>0.93</td>
      <td>-3.16e-01</td>
      <td>1.82</td>
      <td>1.57</td>
      <td>2.10</td>
      <td>-1.14e+00</td>
      <td>-8.39e-01</td>
      <td>-1.02e+00</td>
    </tr>
    <tr>
      <th>l1</th>
      <td>0.00</td>
      <td>-0.00e+00</td>
      <td>0.00</td>
      <td>-0.00e+00</td>
      <td>24.40</td>
      <td>25.16</td>
      <td>25.36</td>
      <td>-0.00e+00</td>
      <td>-0.00e+00</td>
      <td>-0.00e+00</td>
    </tr>
    <tr>
      <th>l1l2</th>
      <td>0.78</td>
      <td>0.00e+00</td>
      <td>0.51</td>
      <td>-0.00e+00</td>
      <td>7.20</td>
      <td>5.71</td>
      <td>8.95</td>
      <td>-1.38e+00</td>
      <td>-0.00e+00</td>
      <td>-4.01e-01</td>
    </tr>
  </tbody>
</table>
</div></section>
<section id="ridge-regression-ell-2-regularization">
<h2>Ridge regression (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)<a class="headerlink" href="#ridge-regression-ell-2-regularization" title="Permalink to this headline">¶</a></h2>
<p>Ridge regression impose a <span class="math notranslate nohighlight">\(\ell_2\)</span> penalty on the coefficients,
i.e. it penalizes with the Euclidean norm of the coefficients while
minimizing SSE. The objective function becomes:</p>
<p>The <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> that minimises <span class="math notranslate nohighlight">\(F_{Ridge}(\mathbf{w})\)</span> can
be found by the following derivation:</p>
<ul class="simple">
<li><p>The solution adds a positive constant to the diagonal of
<span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span> before inversion. This makes the
problem nonsingular, even if <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span> is not of
full rank, and was the main motivation behind ridge regression.</p></li>
<li><p>Increasing <span class="math notranslate nohighlight">\(\lambda\)</span> shrinks the <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>
coefficients toward 0.</p></li>
<li><p>This approach <strong>penalizes</strong> the objective function by the <strong>Euclidian
(:math:`ell_2`) norm</strong> of the coefficients such that solutions with
large coefficients become unattractive.</p></li>
</ul>
<p>The gradient of the loss:</p>
<div class="math notranslate nohighlight">
\[\partial\frac{L(\mathbf{w}, \mathbf{X}, \mathbf{y})}{\partial\mathbf{w}} = 2 (\sum_i \mathbf{x}_i (\mathbf{x}_i \cdot \mathbf{w} - y_i) + \lambda \mathbf{w})\]</div>
</section>
<section id="lasso-regression-ell-1-regularization">
<h2>Lasso regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)<a class="headerlink" href="#lasso-regression-ell-1-regularization" title="Permalink to this headline">¶</a></h2>
<p>Lasso regression penalizes the coefficients by the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm.
This constraint will reduce (bias) the capacity of the learning
algorithm. To add such a penalty forces the coefficients to be small,
i.e. it shrinks them toward zero. The objective function to minimize
becomes:</p>
<p>This penalty forces some coefficients to be exactly zero, providing a
feature selection property.</p>
<section id="sparsity-of-the-ell-1-norm">
<h3>Sparsity of the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm<a class="headerlink" href="#sparsity-of-the-ell-1-norm" title="Permalink to this headline">¶</a></h3>
<section id="occams-razor">
<h4>Occam’s razor<a class="headerlink" href="#occams-razor" title="Permalink to this headline">¶</a></h4>
<p>Occam’s razor (also written as Ockham’s razor, and <strong>lex parsimoniae</strong>
in Latin, which means law of parsimony) is a problem solving principle
attributed to William of Ockham (1287-1347), who was an English
Franciscan friar and scholastic philosopher and theologian. The
principle can be interpreted as stating that <strong>among competing
hypotheses, the one with the fewest assumptions should be selected</strong>.</p>
</section>
<section id="principle-of-parsimony">
<h4>Principle of parsimony<a class="headerlink" href="#principle-of-parsimony" title="Permalink to this headline">¶</a></h4>
<p>The simplest of two competing theories is to be preferred. Definition of
parsimony: Economy of explanation in conformity with Occam’s razor.</p>
<p>Among possible models with similar loss, choose the simplest one:</p>
<ul class="simple">
<li><p>Choose the model with the smallest coefficient vector, i.e. smallest
<span class="math notranslate nohighlight">\(\ell_2\)</span> (<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_2\)</span>) or <span class="math notranslate nohighlight">\(\ell_1\)</span>
(<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_1\)</span>) norm of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>,
i.e. <span class="math notranslate nohighlight">\(\ell_2\)</span> or <span class="math notranslate nohighlight">\(\ell_1\)</span> penalty. See also bias-variance
tradeoff.</p></li>
<li><p>Choose the model that uses the smallest number of predictors. In
other words, choose the model that has many predictors with zero
weights. Two approaches are available to obtain this: (i) Perform a
feature selection as a preprocessing prior to applying the learning
algorithm, or (ii) embed the feature selection procedure within the
learning process.</p></li>
</ul>
</section>
<section id="sparsity-induced-penalty-or-embedded-feature-selection-with-the-ell-1-penalty">
<h4>Sparsity-induced penalty or embedded feature selection with the <span class="math notranslate nohighlight">\(\ell_1\)</span> penalty<a class="headerlink" href="#sparsity-induced-penalty-or-embedded-feature-selection-with-the-ell-1-penalty" title="Permalink to this headline">¶</a></h4>
<p>The penalty based on the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm promotes <strong>sparsity</strong>
(scattered, or not dense): it forces many coefficients to be exactly
zero. This also makes the coefficient vector scattered.</p>
<p>The figure bellow illustrates the OLS loss under a constraint acting on
the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm of the coefficient vector. I.e., it illustrates
the following optimization problem:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \underset{\mathbf{w}}{\text{minimize}} ~&amp; \|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2 \\
    \text{subject to}                 ~&amp; \|\mathbf{w}\|_1 \leq 1.
\end{aligned}\end{split}\]</div>
<figure class="align-default" id="id4">
<img alt="Sparsity of L1 norm" src="../_images/l1_sparse.png" />
<figcaption>
<p><span class="caption-text">Sparsity of L1 norm</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="optimization-issues">
<h3>Optimization issues<a class="headerlink" href="#optimization-issues" title="Permalink to this headline">¶</a></h3>
<p><em>Section to be completed</em></p>
<ul class="simple">
<li><p>No more closed-form solution.</p></li>
<li><p>Convex but not differentiable.</p></li>
<li><p>Requires specific optimization algorithms, such as the fast iterative
shrinkage-thresholding algorithm (FISTA): Amir Beck and Marc
Teboulle, <em>A Fast Iterative Shrinkage-Thresholding Algorithm for
Linear Inverse Problems</em> SIAM J. Imaging Sci., 2009.</p></li>
</ul>
<p>The ridge penalty shrinks the coefficients toward zero. The figure
illustrates: the OLS solution on the left. The <span class="math notranslate nohighlight">\(\ell_1\)</span> and
<span class="math notranslate nohighlight">\(\ell_2\)</span> penalties in the middle pane. The penalized OLS in the
right pane. The right pane shows how the penalties shrink the
coefficients toward zero. The black points are the minimum found in each
case, and the white points represents the true solution used to generate
the data.</p>
<figure class="align-default" id="id5">
<img alt=":math:`\ell_1` and :math:`\ell_2` shrinkages" src="../_images/ols_l1_l2.png" />
<figcaption>
<p><span class="caption-text"><span class="math notranslate nohighlight">\(\ell_1\)</span> and <span class="math notranslate nohighlight">\(\ell_2\)</span> shrinkages</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="elastic-net-regression-ell-1-ell-2-regularization">
<h2>Elastic-net regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)<a class="headerlink" href="#elastic-net-regression-ell-1-ell-2-regularization" title="Permalink to this headline">¶</a></h2>
<p>The Elastic-net estimator combines the <span class="math notranslate nohighlight">\(\ell_1\)</span> and <span class="math notranslate nohighlight">\(\ell_2\)</span>
penalties, and results in the problem to</p>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> acts as a global penalty and <span class="math notranslate nohighlight">\(\rho\)</span> as an
<span class="math notranslate nohighlight">\(\ell_1 / \ell_2\)</span> ratio.</p>
<section id="rational">
<h3>Rational<a class="headerlink" href="#rational" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>If there are groups of highly correlated variables, Lasso tends to
arbitrarily select only one from each group. These models are
difficult to interpret because covariates that are strongly
associated with the outcome are not included in the predictive model.
Conversely, the elastic net encourages a grouping effect, where
strongly correlated predictors tend to be in or out of the model
together.</p></li>
<li><p>Studies on real world data and simulation studies show that the
elastic net often outperforms the lasso, while enjoying a similar
sparsity of representation.</p></li>
</ul>
</section>
</section>
<section id="regression-performance-evaluation-metrics-r-squared-mse-and-mae">
<h2>Regression performance evaluation metrics: R-squared, MSE and MAE<a class="headerlink" href="#regression-performance-evaluation-metrics-r-squared-mse-and-mae" title="Permalink to this headline">¶</a></h2>
<p>Common regression
<a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html">metrics</a>
are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R^2\)</span> : R-squared</p></li>
<li><p>MSE: Mean Squared Error</p></li>
<li><p>MAE: Mean Absolute Error</p></li>
</ul>
<section id="r-squared">
<h3>R-squared<a class="headerlink" href="#r-squared" title="Permalink to this headline">¶</a></h3>
<p>The goodness of fit of a statistical model describes how well it fits a
set of observations. Measures of goodness of fit typically summarize the
discrepancy between observed values and the values expected under the
model in question. We will consider the <strong>explained variance</strong> also
known as the coefficient of determination, denoted <span class="math notranslate nohighlight">\(R^2\)</span>
pronounced <strong>R-squared</strong>.</p>
<p>The total sum of squares, <span class="math notranslate nohighlight">\(SS_\text{tot}\)</span> is the sum of the sum of
squares explained by the regression, <span class="math notranslate nohighlight">\(SS_\text{reg}\)</span>, plus the sum
of squares of residuals unexplained by the regression,
<span class="math notranslate nohighlight">\(SS_\text{res}\)</span>, also called the SSE, i.e. such that</p>
<div class="math notranslate nohighlight">
\[SS_\text{tot} = SS_\text{reg} + SS_\text{res}\]</div>
<figure class="align-default" id="id6">
<img alt="title" src="../_images/Coefficient_of_Determination.png" />
<figcaption>
<p><span class="caption-text">title</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The mean of <span class="math notranslate nohighlight">\(y\)</span> is</p>
<div class="math notranslate nohighlight">
\[\bar{y} = \frac{1}{n}\sum_i y_i.\]</div>
<p>The total sum of squares is the total squared sum of deviations from the
mean of <span class="math notranslate nohighlight">\(y\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[SS_\text{tot}=\sum_i (y_i-\bar{y})^2\]</div>
<p>The regression sum of squares, also called the explained sum of squares:</p>
<div class="math notranslate nohighlight">
\[SS_\text{reg} = \sum_i (\hat{y}_i -\bar{y})^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}_i = \beta x_i + \beta_0\)</span> is the estimated value of
salary <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> given a value of experience <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<p>The sum of squares of the residuals (<strong>SSE, Sum Squared Error</strong>), also
called the residual sum of squares (RSS) is:</p>
<div class="math notranslate nohighlight">
\[SS_\text{res}=\sum_i (y_i - \hat{y_i})^2.\]</div>
<p><span class="math notranslate nohighlight">\(R^2\)</span> is the explained sum of squares of errors. It is the
variance explain by the regression divided by the total variance, i.e.</p>
<div class="math notranslate nohighlight">
\[R^2 = \frac{\text{explained SS}}{\text{total SS}}
    = \frac{SS_\text{reg}}{SS_{tot}}
    = 1 - {SS_{res}\over SS_{tot}}.\]</div>
<p><em>Test</em></p>
<p>Let <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = SS_\text{res} / (n-2)\)</span> be an estimator of
the variance of <span class="math notranslate nohighlight">\(\epsilon\)</span>. The <span class="math notranslate nohighlight">\(2\)</span> in the denominator stems
from the 2 estimated parameters: intercept and coefficient.</p>
<ul class="simple">
<li><p><strong>Unexplained variance</strong>:
<span class="math notranslate nohighlight">\(\frac{SS_\text{res}}{\hat{\sigma}^2} \sim \chi_{n-2}^2\)</span></p></li>
<li><p><strong>Explained variance</strong>:
<span class="math notranslate nohighlight">\(\frac{SS_\text{reg}}{\hat{\sigma}^2} \sim \chi_{1}^2\)</span>. The
single degree of freedom comes from the difference between
<span class="math notranslate nohighlight">\(\frac{SS_\text{tot}}{\hat{\sigma}^2} (\sim \chi^2_{n-1})\)</span> and
<span class="math notranslate nohighlight">\(\frac{SS_\text{res}}{\hat{\sigma}^2} (\sim \chi_{n-2}^2)\)</span>,
i.e. <span class="math notranslate nohighlight">\((n-1) - (n-2)\)</span> degree of freedom.</p></li>
</ul>
<p>The Fisher statistics of the ratio of two variances:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{Explained variance}}{\text{Unexplained variance}} = \frac{SS_\text{reg} / 1}{ SS_\text{res} / (n - 2)} \sim F(1, n-2)\]</div>
<p>Using the <span class="math notranslate nohighlight">\(F\)</span>-distribution, compute the probability of observing a
value greater than <span class="math notranslate nohighlight">\(F\)</span> under <span class="math notranslate nohighlight">\(H_0\)</span>, i.e.:
<span class="math notranslate nohighlight">\(P(x &gt; F|H_0)\)</span>, i.e. the survival function
<span class="math notranslate nohighlight">\((1 - \text{Cumulative Distribution Function})\)</span> at <span class="math notranslate nohighlight">\(x\)</span> of
the given <span class="math notranslate nohighlight">\(F\)</span>-distribution.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">r2</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;r2: </span><span class="si">%.3f</span><span class="s2">, mae: </span><span class="si">%.3f</span><span class="s2">, mse: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">r2</span><span class="p">:</span> <span class="mf">0.050</span><span class="p">,</span> <span class="n">mae</span><span class="p">:</span> <span class="mf">71.834</span><span class="p">,</span> <span class="n">mse</span><span class="p">:</span> <span class="mf">7891.217</span>
</pre></div>
</div>
<p>In pure numpy:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">y_test</span> <span class="o">-</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">y_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
<span class="n">ss_tot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ss_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">res</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ss_res</span> <span class="o">/</span> <span class="n">ss_tot</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">res</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;r2: </span><span class="si">%.3f</span><span class="s2">, mae: </span><span class="si">%.3f</span><span class="s2">, mse: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">r2</span><span class="p">:</span> <span class="mf">0.050</span><span class="p">,</span> <span class="n">mae</span><span class="p">:</span> <span class="mf">71.834</span><span class="p">,</span> <span class="n">mse</span><span class="p">:</span> <span class="mf">7891.217</span>
</pre></div>
</div>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Statistics and Machine Learning in Python</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/python_ecosystem.html">Python ecosystem for data-science</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html#data-analysis-methodology">Data analysis methodology</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html">Import libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#basic-operations">Basic operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#execution-control-statements">Execution control statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#list-comprehensions-iterators-etc">List comprehensions, iterators, etc.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#functions">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#regular-expression">Regular expression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#system-programming">System programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#scripts-and-argument-parsing">Scripts and argument parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#networking">Networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#modules-and-packages">Modules and packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#object-oriented-programming-oop">Object Oriented Programming (OOP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#style-guide-for-python-programming">Style guide for Python programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#documenting">Documenting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#exercises">Exercises</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/scipy_numpy.html">Numpy: arrays and matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/scipy_pandas.html">Pandas: data manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scientific_python/scipy_matplotlib.html">Data visualization: matplotlib &amp; seaborn</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_univ.html">Univariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/stat_univ_lab_brain-volume.html">Lab: Brain volumes study</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_multiv.html">Multivariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/time_series.html">Time series in python</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="decomposition.html">Linear dimension reduction and feature extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="manifold.html">Manifold learning: non-linear dimension reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Clustering</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Linear models for regression problems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#ordinary-least-squares">Ordinary least squares</a></li>
<li class="toctree-l2"><a class="reference internal" href="#linear-regression-with-scikit-learn">Linear regression with scikit-learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overfitting">Overfitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularization-using-penalization-of-coefficients">Regularization using penalization of coefficients</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ridge-regression-ell-2-regularization">Ridge regression (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lasso-regression-ell-1-regularization">Lasso regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#elastic-net-regression-ell-1-ell-2-regularization">Elastic-net regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regression-performance-evaluation-metrics-r-squared-mse-and-mae">Regression performance evaluation metrics: R-squared, MSE and MAE</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="linear_classification.html">Linear models for classification problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/ml_supervized_nonlinear.html">Non-linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/ml_resampling.html">Resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensemble_learning.html">Ensemble learning: bagging, boosting and stacking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/optim_gradient_descent.html">Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/ml_lab_face_recognition.html">Lab: Faces recognition using various learning models</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_backprop_numpy-pytorch-sklearn.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_mlp_mnist_pytorch.html">Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_cnn_cifar10_pytorch.html">Convolutional neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_transfer-learning_cifar10-ants-bees_pytorch.html">Transfer Learning Tutorial</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="clustering.html" title="previous chapter">Clustering</a></li>
      <li>Next: <a href="linear_classification.html" title="next chapter">Linear models for classification problems</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.0.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/machine_learning/linear_regression.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>